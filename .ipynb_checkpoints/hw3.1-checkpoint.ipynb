{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutionary Neural Network Implementation for HOMEWORK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (724, 150))\n",
      "('X_test shape:', (242, 150))\n",
      "(724, 'train samples')\n",
      "(242, 'test samples')\n",
      "('Y_train shape:', (724, 10))\n",
      "('Y_test shape:', (242, 10))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3da21cf20f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# reshape to fit data model (considering the sample and channel dimensions as well)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mreshapedTrainX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainXPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainXPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainXPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mreshapedTestX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestXPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestXPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestXPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#reshapedTrainY = Y_train.reshape((-1,1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# input params\n",
    "batch_size = 32\n",
    "nb_classes = 10\n",
    "nb_epoch = 1000\n",
    "data_augmentation = False \n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 50, 37\n",
    "\n",
    "# the dataset values are single dimension/channel of intensity\n",
    "img_channels = 1\n",
    "\n",
    "# load dataset\n",
    "givenX_train = np.load('X_train.npy')\n",
    "givenY_train = np.load('y_train.npy')\n",
    "givenX_test = np.load('X_test.npy')\n",
    "\n",
    "# split training set into subsets\n",
    "trainX, testX, trainY, testY = train_test_split(givenX_train, givenY_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# use PCA to reduce the train/test data to eigenfaces\n",
    "n_components = 150\n",
    "pca = PCA(n_components = n_components, svd_solver='randomized', whiten=True).fit(trainX)\n",
    "trainXPCA = pca.transform(trainX)\n",
    "testXPCA = pca.transform(testX)\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "print('X_train shape:', trainXPCA.shape)\n",
    "print('X_test shape:', testXPCA.shape)\n",
    "print(trainXPCA.shape[0], 'train samples')\n",
    "print(testXPCA.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(trainY, nb_classes)\n",
    "Y_test = np_utils.to_categorical(testY, nb_classes)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)\n",
    "\n",
    "# normalize\n",
    "#trainXPCA = trainXPCA.astype('float32')\n",
    "#testXPCA = testXPCA.astype('float32')\n",
    "#trainXPCA /= 255\n",
    "#testXPCA /= 255\n",
    "\n",
    "# reshape to fit data model (considering the sample and channel dimensions as well)\n",
    "reshapedTrainX = trainXPCA.reshape((trainXPCA.shape[0],) + (1, trainXPCA.shape[1]))\n",
    "reshapedTestX = testXPCA.reshape((testXPCA.shape[0],) + (1, testXPCA.shape[1]))\n",
    "#reshapedTrainY = Y_train.reshape((-1,1))\n",
    "#reshapedTestY = Y_test.reshape((-1,1))\n",
    "\n",
    "# sequentially apply filters\n",
    "model = Sequential()\n",
    "\n",
    "# declare convolution shape with 32 convolution filters\n",
    "model.add(Convolution1D(32, 1, border_mode='same', input_shape=(1, trainXPCA.shape[1])))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(32, 1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_length=1))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(64, 1, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(64, 1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_length=1))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(reshapedTrainX, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              validation_data=(reshapedTestX, Y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "\n",
    "    # this will do preprocessing and realtime data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied)\n",
    "    datagen.fit(reshapedTrainX)\n",
    "\n",
    "    # fit the model on the batches generated by datagen.flow()\n",
    "    model.fit_generator(datagen.flow(reshapedTrainX, Y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        samples_per_epoch=reshapedTrainX.shape[0],\n",
    "                        nb_epoch=nb_epoch,\n",
    "                        validation_data=(reshapedTestX, Y_test))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
